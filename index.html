<!-- HTML header for doxygen 1.8.3.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3.1"/>
<title>CUB: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra_stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-38890655-1']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">CUB
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(10)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(11)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">CUB Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#sec0">(1) What is CUB?</a></li>
<li class="level1"><a href="#sec2">(2) Recent news</a></li>
<li class="level1"><a href="#sec3">(3) A simple example</a></li>
<li class="level1"><a href="#sec4">(4) Why do you need CUB?</a></li>
<li class="level1"><a href="#sec5">(5) Where is CUB positioned in the CUDA ecosystem?</a></li>
<li class="level1"><a href="#sec6">(6) How does CUB work?</a><ul><li class="level2"><a href="#sec3sec1">6.1 &nbsp;&nbsp; C++ templates</a></li>
<li class="level2"><a href="#sec3sec2">6.2 &nbsp;&nbsp; Reflective type structure</a></li>
<li class="level2"><a href="#sec3sec3">6.3 &nbsp;&nbsp; Flexible data mapping</a></li>
</ul>
</li>
<li class="level1"><a href="#sec7">(7) Contributors</a></li>
<li class="level1"><a href="#sec8">(8) Open Source License</a></li>
</ul>
</div>
<div class="textblock"> 
<a href="https://github.com/NVlabs/CUB/archive/0.9.zip"><img src="download-icon.png" style="position:relative; bottom:-10px;"/></a>
&nbsp;&nbsp;
<a href="https://github.com/NVlabs/CUB/archive/0.9.zip">Download CUB!</a>
<br>
<a href="https://github.com/NVlabs/CUB"><img src="github-icon-747d8b799a48162434b2c0595ba1317e.png" style="position:relative; bottom:-10px;"/></a>
&nbsp;&nbsp;
<a href="https://github.com/NVlabs/CUB">Fork CUB at GitHub!</a>
<br>
<a href="http://groups.google.com/group/cub-users"><img src="groups-icon.png" style="position:relative; bottom:-10px;"/></a>
&nbsp;&nbsp;
<a href="http://groups.google.com/group/cub-users">Join the cub-users discussion forum!</a>
<h1><a class="anchor" id="sec0"></a>
(1) What is CUB?</h1>
<dl class="section user"><dt></dt><dd>CUB is a library of high performance threadblock primitives and other utilities for CUDA kernel programming. CUB enhances productivity, performance, and portability by providing an abstraction layer over complex threadblock, warp, and thread-level operations.</dd></dl>
<dl class="section user"><dt></dt><dd>CUB's primitives are not bound to any particular width-of-parallelism or data type. This allows them to be flexible and tunable to fit your kernel needs. Thus CUB is <a href="index.html"><b>C</b>uda <b>U</b>n<b>b</b>ound</a>.</dd></dl>
<div class="image">
<img src="simt_abstraction.png" alt="simt_abstraction.png"/>
</div>
<dl class="section user"><dt></dt><dd>Browse our collections of:<ul>
<li><a href="group___simt_coop.html"><b>Cooperative primitives</b></a>:<ul>
<li>Threadblock operations (e.g., BlockRadixSort, BlockScan, BlockReduce, etc.)</li>
<li>Warp operations (e.g., WarpScan, etc.)</li>
<li>etc.</li>
</ul>
</li>
<li><a href="group___simt_utils.html"><b>SIMT utilities</b></a>:<ul>
<li>Tile-based I/O utilities for performing vectorized|coalesced data movement of blocked|striped data tiles</li>
<li>Low-level thread-I/O using cache-modifiers (e.g., ThreadLoad &amp; ThreadStore intrinsics)</li>
<li>Abstractions for threadblock work distribution (GridQueue, GridEvenShare, etc.)</li>
<li>etc.</li>
</ul>
</li>
<li><a href="group___host_util.html"><b>Host utilities</b></a>:<ul>
<li>Caching allocator for quick management of device temporaries</li>
<li>Device reflection</li>
<li>etc.</li>
</ul>
</li>
</ul>
</dd></dl>
<h1><a class="anchor" id="sec2"></a>
(2) Recent news</h1>
<dl class="section user"><dt></dt><dd><ul>
<li><a href="https://github.com/NVlabs/CUB/archive/0.9.zip"><b><em>CUB v0.9 "preview" release</em></b></a> (3/6/2013). CUB is the first durable, high-performance library of cooperative threadblock, warp, and thread primitives for CUDA kernel programming. More primitives and examples coming soon!</li>
</ul>
</dd></dl>
<h1><a class="anchor" id="sec3"></a>
(3) A simple example</h1>
<dl class="section user"><dt></dt><dd>The following code snippet illustrates a simple CUDA kernel for sorting a threadblock's data:</dd></dl>
<dl class="section user"><dt></dt><dd><div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="cub_8cuh.html">cub.cuh</a>&gt;</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// An tile-sorting CUDA kernel</span></div>
<div class="line"><span class="keyword">template</span> &lt;</div>
<div class="line">     <span class="keywordtype">int</span>         BLOCK_THREADS,              <span class="comment">// Threads per threadblock</span></div>
<div class="line">     <span class="keywordtype">int</span>         ITEMS_PER_THREAD,           <span class="comment">// Items per thread</span></div>
<div class="line">     <span class="keyword">typename</span>    T&gt;                          <span class="comment">// Numeric data type</span></div>
<div class="line">__global__ <span class="keywordtype">void</span> TileSortKernel(T *d_in, T *d_out)</div>
<div class="line">{</div>
<div class="line">     <span class="keyword">using namespace </span>cub;</div>
<div class="line">     <span class="keyword">const</span> <span class="keywordtype">int</span> TILE_SIZE = BLOCK_THREADS * ITEMS_PER_THREAD;</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Parameterize cub::BlockRadixSort for the parallel execution context</span></div>
<div class="line">     <span class="keyword">typedef</span> <a class="code" href="classcub_1_1_block_radix_sort.html" title="BlockRadixSort provides variants of parallel radix sorting across a CUDA threadblock. .">BlockRadixSort&lt;T, BLOCK_THREADS&gt;</a> BlockRadixSort;</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Declare the shared memory needed by BlockRadixSort</span></div>
<div class="line">     __shared__ <span class="keyword">typename</span> <a class="code" href="classcub_1_1_block_radix_sort.html#a495e63ab526ce35e6dfce9fb5206746c" title="The operations exposed by BlockRadixSort require shared memory of this type. This opaque storage can ...">BlockRadixSort::SmemStorage</a> smem_storage;</div>
<div class="line"></div>
<div class="line">     <span class="comment">// A segment of data items per thread</span></div>
<div class="line">     T data[ITEMS_PER_THREAD];</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Load a tile of data using vector-load instructions</span></div>
<div class="line">     <a class="code" href="group___simt_utils.html#gaea8200ef976bb588c569e039ea79005c" title="Load a tile of items across a threadblock directly using the specified cache modifier.">BlockLoadVectorized</a>(data, d_in + (blockIdx.x * TILE_SIZE));</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Sort data in ascending order</span></div>
<div class="line">     <a class="code" href="classcub_1_1_block_radix_sort.html#abdbfda59c129946222ab10d2e2e6f6f5" title="Performs a threadblock-wide radix sort over a blocked arrangement of keys.">BlockRadixSort::SortBlocked</a>(smem_storage, data);</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Store the sorted tile using vector-store instructions</span></div>
<div class="line">     <a class="code" href="group___simt_utils.html#ga013c3ab8214854f45e8d678958e7dde9" title="Store a tile of items across a threadblock directly using the specified cache modifier.">BlockStoreVectorized</a>(data, d_out + (blockIdx.x * TILE_SIZE));</div>
<div class="line">}</div>
</div><!-- fragment --></dd></dl>
<dl class="section user"><dt></dt><dd>The <a class="el" href="classcub_1_1_block_radix_sort.html" title="BlockRadixSort provides variants of parallel radix sorting across a CUDA threadblock. .">cub::BlockRadixSort</a> type performs a cooperative radix sort across the threadblock's data items. Its implementation is parameterized by the number of threadblock threads and the aggregate data type <code>T</code>, and is specialized for the underlying architecture.</dd></dl>
<dl class="section user"><dt></dt><dd>Once instantiated, the <a class="el" href="classcub_1_1_block_radix_sort.html" title="BlockRadixSort provides variants of parallel radix sorting across a CUDA threadblock. .">cub::BlockRadixSort</a> type exposes an opaque <a class="el" href="classcub_1_1_block_radix_sort.html#a495e63ab526ce35e6dfce9fb5206746c" title="The operations exposed by BlockRadixSort require shared memory of this type. This opaque storage can ...">cub::BlockRadixSort::SmemStorage</a> member type. The threadblock uses this storage type to allocate the shared memory needed by the primitive. This storage type can be aliased or <code>union</code>'d with other types so that the shared memory can be reused for other purposes.</dd></dl>
<dl class="section user"><dt></dt><dd>Furthermore, the kernel uses CUB's primitives for vectorizing global loads and stores. For example, <code>ld.global.v4.s32</code> PTX instructions will be generated when <code>T</code> = <code>int</code> and <code>ITEMS_PER_THREAD</code> is a multiple of 4.</dd></dl>
<h1><a class="anchor" id="sec4"></a>
(4) Why do you need CUB?</h1>
<dl class="section user"><dt></dt><dd>Kernel development is perhaps the most challenging, time-consuming, and costly aspect of CUDA programming. It is where the complexity of parallelism is expressed. Kernel code is often the most performance-sensitive and difficult-to-maintain layer in the CUDA software stack. This is particularly true for complex parallel algorithms having intricate dependences between threads.</dd></dl>
<dl class="section user"><dt></dt><dd>However, with the exception of CUB, there are few (if any) software libraries of reusable kernel primitives. In the CUDA ecosystem, CUB is unique in this regard. As a SIMT library and software abstraction layer, CUB provides:<ol type="1">
<li><b>Simplicity of composition.</b> Parallel CUB primitives can be simply sequenced together in kernel code. (This convenience is analogous to programming with <a href="http://http://thrust.github.com/"><b>Thrust</b></a> primitives in the host program.)</li>
<li><b>High performance.</b> CUB simplifies high performance kernel development by taking care to implement the fastest available algorithms, strategies, and techniques. (So you don't have to.)</li>
<li><b>Performance-portability.</b> CUB primitives are specialized to match the target hardware. Furthermore, CUB continually evolves to accommodate new algorithmic developments, hardware instructions, etc. Instead of re-writing code by hand, CUDA kernels can be made future-proof by simply targeting CUB primitives.</li>
<li><b>Simplicity of performance tuning.</b> CUB primitives provide parallel abstractions whose performance behavior can be statically tuned. For example, most CUB primitives support alternative algorithmic strategies and variable grain sizes (i.e., threads per threadblock, items per thread, etc.).</li>
<li><b>Robustness and durability.</b> CUB primitives are designed to function properly for arbitrary data types and widths-of-parallelism (not just for the built-in C++ types and power-of-two threadblocks).</li>
</ol>
</dd></dl>
<h1><a class="anchor" id="sec5"></a>
(5) Where is CUB positioned in the CUDA ecosystem?</h1>
<dl class="section user"><dt></dt><dd>CUDA's programming model exposes three different levels of execution and their corresponding abstraction layers (i.e., the "black boxes" shown in the software stacks below):</dd></dl>
<table  border="0px" cellpadding="10px" cellspacing="0px">
<tr>
<td width="50%"><ul>
<li><b>Grid kernel (scalar)</b>. A single thread invokes a CUDA grid to perform some data parallel function. This is the highest and most common level of CUDA software abstraction. Programmers do not have to reason about parallel CUDA threads. Libraries targeting this level include:<ul>
<li><a href="https://developer.nvidia.com/cublas"><b>CUBLAS</b></a></li>
<li><a href="https://developer.nvidia.com/cufft"><b>CUFFT</b></a></li>
<li><a href="https://developer.nvidia.com/cusparse"><b>CUSPARSE</b></a></li>
<li><a href="http://thrust.github.com/"><b>Thrust</b></a>  </li>
</ul>
</li>
</ul>
</td><td width="50%"> 
   <a href="kernel_abstraction.png"><center><img src="kernel_abstraction.png" width="100%"/></center></a>
     </td></tr>
<tr>
<td><ul>
<li><b>Threadblock / warp (SIMT)</b>. A threadblock or warp of threads collectively invokes some cooperative function. This is the least common level of CUDA software reuse. Libraries targeting this level include:<ul>
<li><a href="index.html"><b>CUB</b></a>  </li>
</ul>
</li>
</ul>
</td><td> 
   <a href="simt_abstraction.png"><center><img src="simt_abstraction.png" width="100%"/></center></a>
     </td></tr>
<tr>
<td><ul>
<li><b>Device thread (scalar)</b>. A single CUDA thread invokes some scalar function. This is the lowest level of CUDA software abstraction. Programmers do not have to reason about the interaction of parallel CUDA threads. Libraries targeting this level include:<ul>
<li><b>CUDA API</b> (e.g., <code>text1D()</code>, <code>atomicAdd()</code>, <code>popc()</code>, etc.)</li>
<li><a href="index.html"><b>CUB</b></a>  </li>
</ul>
</li>
</ul>
</td><td> 
   <a href="devfun_abstraction.png"><center><img src="devfun_abstraction.png" width="100%"/></center></a>
     </td></tr>
</table>
<h1><a class="anchor" id="sec6"></a>
(6) How does CUB work?</h1>
<dl class="section user"><dt></dt><dd>CUB leverages the following programming idioms:<ol type="1">
<li><a href="index.html#sec3sec1"><b>C++ templates</b></a></li>
<li><a href="index.html#sec3sec2"><b>Reflective type structure</b></a></li>
<li><a href="index.html#sec3sec3"><b>Flexible data mapping</b></a></li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sec3sec1"></a>
6.1    C++ templates</h2>
<dl class="section user"><dt></dt><dd>As a SIMT library, CUB must be flexible enough to accommodate a wide spectrum of <em>parallel execution contexts</em>, i.e., specific:<ul>
<li>Data types</li>
<li>Widths of parallelism (threadblock threads)</li>
<li>Grain sizes (data items per thread)</li>
<li>Underlying architectures (special instructions, warp width, rules for bank conflicts, etc.)</li>
<li>Tuning requirements (e.g., latency vs. throughput)</li>
</ul>
</dd></dl>
<dl class="section user"><dt></dt><dd>To provide this flexibility, CUB is implemented as a C++ template library. C++ templates are a way to write generic algorithms and data structures. There is no need to build CUB separately. You simply #<code>include</code> the <code><a class="el" href="cub_8cuh.html">cub.cuh</a></code> header file into your <code>.cu</code> or <code>.cpp</code> sources and compile with CUDA's <code>nvcc</code> compiler.</dd></dl>
<h2><a class="anchor" id="sec3sec2"></a>
6.2    Reflective type structure</h2>
<dl class="section user"><dt></dt><dd>Cooperation requires shared memory for communicating between threads. However, the specific size and layout of the memory needed by a given primitive will be specific to the details of its parallel execution context (e.g., how many threads are calling into it, how many items per thread, etc.). Furthermore, this shared memory must be allocated externally to the component if it is to be reused elsewhere by the threadblock.</dd></dl>
<dl class="section user"><dt></dt><dd><div class="fragment"><div class="line"><span class="comment">// Parameterize a BlockScan type for use with 128 threads</span></div>
<div class="line"><span class="comment">// and 4 items per thread</span></div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="classcub_1_1_block_scan.html" title="BlockScan provides variants of parallel prefix scan (and prefix sum) across a CUDA threadblock...">cub::BlockScan&lt;unsigned int, 128, 4&gt;</a> BlockScan;</div>
<div class="line"></div>
<div class="line"><span class="comment">// Declare shared memory for BlockScan</span></div>
<div class="line">__shared__ <span class="keyword">typename</span> BlockScan::SmemStorage smem_storage;</div>
<div class="line"></div>
<div class="line"><span class="comment">// A segment of consecutive input items per thread</span></div>
<div class="line"><span class="keywordtype">int</span> data[4];</div>
<div class="line"></div>
<div class="line"><span class="comment">// Obtain data in blocked order</span></div>
<div class="line">...</div>
<div class="line"></div>
<div class="line"><span class="comment">// Perform an exclusive prefix sum across the tile of data</span></div>
<div class="line">BlockScan::ExclusiveSum(smem_storage, data, data);</div>
</div><!-- fragment --></dd></dl>
<dl class="section user"><dt></dt><dd>To address this issue, we encapsulate cooperative procedures within <em>reflective type structure</em> (C++ classes). As illustrated in the <a class="el" href="classcub_1_1_block_scan.html" title="BlockScan provides variants of parallel prefix scan (and prefix sum) across a CUDA threadblock...">cub::BlockScan</a> example above, these primitives are C++ classes with interfaces that expose both (1) procedural methods as well as (2) the opaque shared memory types needed for their operation.</dd></dl>
<h2><a class="anchor" id="sec3sec3"></a>
6.3    Flexible data mapping</h2>
<dl class="section user"><dt></dt><dd>We often design kernels such that each threadblock is assigned a "tile" of data items for processing. When the tile size equals the threadblock size, the mapping of data onto threads is straightforward (1:1 items to threadblock threads). Alternatively, it is often desirable to processes more than one datum per thread. When doing so, we must decide how to partition this "tile" of items across the threadblock.</dd></dl>
<dl class="section user"><dt></dt><dd>CUB primitives support the following data arrangements:<ul>
<li><b><em>Blocked arrangement</em></b>. The aggregate tile of items is partitioned evenly across threads in "blocked" fashion with thread<sub><em>i</em></sub> owning the <em>i</em><sup>th</sup> segment of consecutive elements.</li>
<li><b><em>Striped arrangement</em></b>. The aggregate tile of items is partitioned across threads in "striped" fashion, i.e., the <code>ITEMS_PER_THREAD</code> items owned by each thread have logical stride <code>BLOCK_THREADS</code> between them. <br/>
<br/>
 <div class="image">
<img src="thread_data_1.png" alt="thread_data_1.png"/>
</div>
 <div class="centercaption">Blocked vs. striped arrangements with <code>BLOCK_THREADS</code> = 4 and <code>ITEMS_PER_THREAD</code> = 2, emphasis on items owned by <em>thread</em><sub>0</sub></div> <br/>
</li>
</ul>
</dd></dl>
<dl class="section user"><dt></dt><dd>The benefits of processing multiple items per thread (a.k.a., <em>register blocking</em>, <em>granularity coarsening</em>, etc.) include:<ul>
<li><b>Algorithmic efficiency</b>. Sequential work over multiple items in thread-private registers is cheaper than synchronized, cooperative work through shared memory spaces</li>
<li><b>Data occupancy</b>. The number of items that can be resident on-chip in thread-private register storage is often greater than the number of schedulable threads</li>
<li><b>Instruction-level parallelism</b>. Multiple items per thread also facilitates greater ILP for improved throughput and utilization</li>
</ul>
</dd></dl>
<dl class="section user"><dt></dt><dd>The <a class="el" href="classcub_1_1_block_exchange.html" title="BlockExchange provides operations for reorganizing the partitioning of ordered data across a CUDA thr...">cub::BlockExchange</a> primitive provides operations for converting between blocked and striped arrangements. Blocked arrangements are often desirable for algorithmic benefits (where long sequences of items can be processed sequentially within each thread). Striped arrangements are often desirable for data movement through global memory (where read/write coalescing is a important performance consideration).</dd></dl>
<h1><a class="anchor" id="sec7"></a>
(7) Contributors</h1>
<dl class="section user"><dt></dt><dd>CUB is developed as an open-source project by <a href="http://research.nvidia.com">NVIDIA Research</a>. The primary contributor is <a href="http://github.com/dumerrill">Duane Merrill</a>.</dd></dl>
<h1><a class="anchor" id="sec8"></a>
(8) Open Source License</h1>
<dl class="section user"><dt></dt><dd>CUB is available under the "New BSD" open-source license:</dd></dl>
<dl class="section user"><dt></dt><dd><div class="fragment"><div class="line">Copyright (c) 2011, Duane Merrill.  All rights reserved.</div>
<div class="line">Copyright (c) 2011-2013, NVIDIA CORPORATION.  All rights reserved.</div>
<div class="line"></div>
<div class="line">Redistribution and use in source and binary forms, with or without</div>
<div class="line">modification, are permitted provided that the following conditions are met:</div>
<div class="line">    * Redistributions of source code must retain the above copyright</div>
<div class="line">      notice, <span class="keyword">this</span> list of conditions and the following disclaimer.</div>
<div class="line">    * Redistributions in binary form must reproduce the above copyright</div>
<div class="line">      notice, <span class="keyword">this</span> list of conditions and the following disclaimer in the</div>
<div class="line">      documentation and/or other materials provided with the distribution.</div>
<div class="line">    * Neither the name of the NVIDIA CORPORATION nor the</div>
<div class="line">      names of its contributors may be used to endorse or promote products</div>
<div class="line">      derived from <span class="keyword">this</span> software without specific prior written permission.</div>
<div class="line"></div>
<div class="line">THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS <span class="stringliteral">&quot;AS IS&quot;</span> AND</div>
<div class="line">ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED</div>
<div class="line">WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE</div>
<div class="line">DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY</div>
<div class="line">DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES</div>
<div class="line">(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;</div>
<div class="line">LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND</div>
<div class="line">ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT</div>
<div class="line">(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS</div>
<div class="line">SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</div>
</div><!-- fragment --> </dd></dl>
</div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.3.1-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Thu Mar 7 2013 12:42:26 for CUB by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.3.1
<br>
&copy; 2013 NVIDIA Corporation
</small></address>
</body>
</html>
